## 2025-04-23

### What I changed

1. Simplified schema.

### Why it matters

1. Make model response simpler.

### Measurements

- Prompt tokens: 1080
- Call latency: 6.1 s (average of 3 runs)

### Open questions

- Parallel vs. per‑file batching?
- Keep or drop ModuleEdit now?

### Next actions (tomorrow)

- Reuse OpenAI client
- Prototype per‑file chunking

### TODO

- Figure out how to not waste tokens on docs that are alreayd good. Right now I'm
  having the model just echo them back. That's token inefficient and error prone.
